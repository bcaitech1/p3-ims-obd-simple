{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc41371-9527-4ed0-b17c-06b91a0e8837",
   "metadata": {},
   "source": [
    "# PSP Network\n",
    "Pyramid Scene Parsing Network\n",
    "\n",
    "- [paper: Pyramid Scene Parsing Network](https://arxiv.org/abs/1612.01105)\n",
    "- [github: PyTorch Semantic Segmentation](https://github.com/hszhao/semseg)\n",
    "\n",
    "## Contributions\n",
    "- Propose a pyramid scene parsing network to embed difficult scenery context features in an FCN based pixel prediction framework.\n",
    "- Develop an effective optimization strategy for deep ResNet based on deeply supervised loss.\n",
    "    - Use auxiliary loss and master branch's softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f189690-fd21-4eb9-997a-ebdf3570530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9c849e-f461-4dab-a94d-ecefcd1cd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import cv2\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db5e9b4e-b1b0-4a12-8091-ae751c392031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaegyeong\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">morning-glade-13</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/boostcamp-simple/p3-img-seg\" target=\"_blank\">https://wandb.ai/boostcamp-simple/p3-img-seg</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/boostcamp-simple/p3-img-seg/runs/be4e6c6b\" target=\"_blank\">https://wandb.ai/boostcamp-simple/p3-img-seg/runs/be4e6c6b</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/code/wandb/run-20210428_110904-be4e6c6b</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init(project='p3-img-seg', entity='boostcamp-simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8d3f51-b1d0-440d-9886-29c04a1d3f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla P40\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d857f5-eea5-4319-93e1-dcffafc75479",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 세팅 및 seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a3303e5-b7ae-49b4-b2ba-0fda56cbf6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config\n",
    "\n",
    "config.update({\n",
    "    \"batch_size\": 16,\n",
    "    \"num_epochs\": 10,\n",
    "    \"dropout\": 0.2,\n",
    "    \"architecture\": \"PSP with ResNet\",\n",
    "    \"random_seed\": 21,\n",
    "\n",
    "    \"weight_decay\": 0.0001,\n",
    "    \"momentum\": 0.9,\n",
    "    \"zoom_factor\": 8,\n",
    "    \"aux_weight\": 0.4,\n",
    "    \"ignore_label\": 255,\n",
    "    \"base_lr\": 0.01,\n",
    "    \"power\": 0.9,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456bf054-77dc-49c3-bb80-d0841cabc2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = config.random_seed\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdc0a3-91d0-483b-a310-13a86e5579af",
   "metadata": {},
   "source": [
    "## 데이터 전처리 함수 정의 (Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43329b95-7ecf-4d32-ac54-f47a8c04fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of super categories: 11\n",
      "Number of categories: 11\n",
      "Number of annotations: 21116\n",
      "Number of images: 2617\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/opt/ml/input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'\n",
    "\n",
    "# Read annotations\n",
    "with open(anns_file_path, 'r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "categories = dataset['categories']\n",
    "anns = dataset['annotations']\n",
    "imgs = dataset['images']\n",
    "nr_cats = len(categories)\n",
    "nr_annotations = len(anns)\n",
    "nr_images = len(imgs)\n",
    "\n",
    "# Load categories and super categories\n",
    "cat_names = []\n",
    "super_cat_names = []\n",
    "super_cat_ids = {}\n",
    "super_cat_last_name = ''\n",
    "nr_super_cats = 0\n",
    "for cat_it in categories:\n",
    "    cat_names.append(cat_it['name'])\n",
    "    super_cat_name = cat_it['supercategory']\n",
    "    # Adding new supercat\n",
    "    if super_cat_name != super_cat_last_name:\n",
    "        super_cat_names.append(super_cat_name)\n",
    "        super_cat_ids[super_cat_name] = nr_super_cats\n",
    "        super_cat_last_name = super_cat_name\n",
    "        nr_super_cats += 1\n",
    "\n",
    "print('Number of super categories:', nr_super_cats)\n",
    "print('Number of categories:', nr_cats)\n",
    "print('Number of annotations:', nr_annotations)\n",
    "print('Number of images:', nr_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b556ce-04f1-4248-b4ba-b13e872cd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count annotations\n",
    "cat_histogram = np.zeros(nr_cats,dtype=int)\n",
    "for ann in anns:\n",
    "    cat_histogram[ann['category_id']] += 1\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({'Categories': cat_names, 'Number of annotations': cat_histogram})\n",
    "df = df.sort_values('Number of annotations', 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91d6c899-2472-483c-8d82-51b0eef4fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category labeling \n",
    "sorted_temp_df = df.sort_index()\n",
    "\n",
    "# background = 0 에 해당되는 label 추가 후 기존들을 모두 label + 1 로 설정\n",
    "sorted_df = pd.DataFrame([\"Backgroud\"], columns = [\"Categories\"])\n",
    "sorted_df = sorted_df.append(sorted_temp_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0649891a-e6f8-4519-9e80-62fefaf79239",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = list(sorted_df.Categories)\n",
    "\n",
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6b431-45af-4288-a322-74a19345b826",
   "metadata": {},
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0166f41-bf0a-4855-a712-a2ef32ce368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.73s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.44s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            A.Resize(473, 473),\n",
    "                            ToTensorV2(),\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          A.Resize(473, 473),\n",
    "                          ToTensorV2(),\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           A.Resize(473, 473),\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=config.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=config.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=0,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=config.batch_size,\n",
    "                                          num_workers=0,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4339e-d76c-42d8-acb5-0f1c34929e7c",
   "metadata": {},
   "source": [
    "## PSPNet (ResNet50 imageNet weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7efc9480-3a47-4096-97df-6a538ec071d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPM(nn.Module):\n",
    "    \"\"\"\n",
    "    Pyramid Pooling Module\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, reduction_dim, bins):\n",
    "        super(PPM, self).__init__()\n",
    "        self.features = []\n",
    "        for bin in bins:\n",
    "            self.features.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(bin),\n",
    "                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(reduction_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.features = nn.ModuleList(self.features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        out = [x]\n",
    "        for f in self.features:\n",
    "            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n",
    "        return torch.cat(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f48fe77-294e-4484-b480-eb19b5032645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, layers=50, bins=(1, 2, 3, 6), dropout=0.1, classes=12, zoom_factor=8, use_ppm=True,\n",
    "                criterion=nn.CrossEntropyLoss(ignore_index=225), pretrained=True):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            layer       : depth of resnet\n",
    "            bins        : pooling kernel size\n",
    "            dropout     : ratio of dropout\n",
    "            classes     : number of classes\n",
    "            zoom_factor : ?\n",
    "            use_ppm     : Whether or not to use pyramid pooling module\n",
    "            criterion   : loss function\n",
    "            pretrained  : Whether or not to use pretrained model\n",
    "        \"\"\"\n",
    "        super(PSPNet, self).__init__()\n",
    "        assert layers in [50, 101, 152]\n",
    "        assert 2048 % len(bins) == 0\n",
    "        assert classes > 1\n",
    "        assert zoom_factor in [1, 2, 4, 8]\n",
    "        self.zoom_factor = zoom_factor\n",
    "        self.use_ppm = use_ppm\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        if layers == 50:\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "        elif layers == 101:\n",
    "            resnet = models.resnet101(pretrained=pretrained)\n",
    "        else:\n",
    "            resnet = models.resnet152(pretrained=pretrained)\n",
    "            \n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "        \n",
    "        # layer3과 layer4의 모든 Convolution에 dilation\n",
    "        for n, m in self.layer3.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "                \n",
    "        for n, m in self.layer4.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "                \n",
    "        fea_dim = 2048\n",
    "        if use_ppm:\n",
    "            self.ppm = PPM(fea_dim, int(fea_dim/len(bins)), bins)\n",
    "            fea_dim *= 2\n",
    "            \n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Conv2d(fea_dim, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=dropout),\n",
    "            nn.Conv2d(512, classes, kernel_size=1)\n",
    "        )\n",
    "        if self.training:  # Boolean represents whether this module is in training or evaluation mode.\n",
    "            self.aux = nn.Sequential(\n",
    "                nn.Conv2d(1024, 256, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout2d(p=dropout),\n",
    "                nn.Conv2d(256, classes, kernel_size=1)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x, y=None):\n",
    "        x_size = x.size()\n",
    "        assert (x_size[2]-1) % 8 == 0 and (x_size[3]-1) % 8 == 0\n",
    "        h = int((x_size[2] - 1) / 8 * self.zoom_factor + 1)\n",
    "        w = int((x_size[3] - 1) / 8 * self.zoom_factor + 1)\n",
    "\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x_tmp = self.layer3(x)\n",
    "        x = self.layer4(x_tmp)\n",
    "        if self.use_ppm:\n",
    "            x = self.ppm(x)\n",
    "        x = self.cls(x)\n",
    "        if self.zoom_factor != 1:\n",
    "            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)\n",
    "\n",
    "        if self.training:\n",
    "            aux = self.aux(x_tmp)\n",
    "            if self.zoom_factor != 1:\n",
    "                aux = F.interpolate(aux, size=(h, w), mode='bilinear', align_corners=True)\n",
    "            main_loss = self.criterion(x, y)\n",
    "            aux_loss = self.criterion(aux, y)\n",
    "            return x.max(1)[1], main_loss, aux_loss\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d00956-9910-4d2e-a975-61f726d5c8bf",
   "metadata": {},
   "source": [
    "## train, validation, test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a00da04f-6f82-4faf-bae3-8408905ab082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c222b0-f642-4f39-b58a-fc434db87e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectionAndUnionGPU(output, target, K, ignore_index=255):\n",
    "    # 'K' classes, output and target sizes are N or N * L or N * H * W, each value in range 0 to K - 1.\n",
    "    assert (output.dim() in [1, 2, 3])\n",
    "    assert output.shape == target.shape\n",
    "    output = output.view(-1)\n",
    "    target = target.view(-1)\n",
    "    output[target == ignore_index] = ignore_index\n",
    "    intersection = output[output == target]\n",
    "    area_intersection = torch.histc(intersection.float(), bins=K, min=0, max=K-1)\n",
    "    area_output = torch.histc(output.float(), bins=K, min=0, max=K-1)\n",
    "    area_target = torch.histc(target.float(), bins=K, min=0, max=K-1)\n",
    "    area_union = area_output + area_target - area_intersection\n",
    "    return area_intersection, area_union, area_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2205f564-6a07-445a-a2cf-613f090341d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_learning_rate(base_lr, curr_iter, max_iter, power=0.9):\n",
    "    \"\"\"poly learning rate policy\"\"\"\n",
    "    lr = base_lr * (1 - float(curr_iter) / max_iter) ** power\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "067ad14b-ea60-4d13-a986-5e6bf930c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, criterion, optimizer, epoch, saved_dir):\n",
    "    main_loss_meter = AverageMeter()\n",
    "    aux_loss_meter = AverageMeter()\n",
    "    loss_meter = AverageMeter()\n",
    "    intersection_meter = AverageMeter()\n",
    "    union_meter = AverageMeter()\n",
    "    target_meter = AverageMeter()\n",
    "    \n",
    "    classes = 12\n",
    "    print_freq = 100\n",
    "    index_split = 0\n",
    "    val_every = 1\n",
    "    best_mIoU = 0\n",
    "\n",
    "    max_iter = config.num_epochs * len(train_loader)\n",
    "    for i, (input, target, _) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        if config.zoom_factor != 8:\n",
    "            h = int((target.size()[1] - 1) / 8*config.zoom_factor + 1)\n",
    "            w = int((target.size()[2] - 1) / 8*config.zoom_factor + 1)\n",
    "            # 'nearest' mode doesn't support align_corners mode and 'bilinear' mode is fine for downsampling\n",
    "            target = F.interpolate(target.unsqueeze(1).float(), size=(h, w), mode='bilinear', align_corners=True).squeeze(1).long()\n",
    "        \n",
    "        input = torch.stack(input)       # (batch, channel, height, width)\n",
    "        target = torch.stack(target).long()\n",
    "        \n",
    "        output, main_loss, aux_loss = model(input, target)\n",
    "        \n",
    "        main_loss, aux_loss = torch.mean(main_loss), torch.mean(aux_loss)\n",
    "        loss = main_loss + config.aux_weight * aux_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n = input.size(0)\n",
    "        intersection, union, target = intersectionAndUnionGPU(output, target, classes)\n",
    "        \n",
    "        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()\n",
    "        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)\n",
    "        \n",
    "        accuracy = sum(intersection_meter.val) / (sum(target_meter.val) + 1e-10)\n",
    "        main_loss_meter.update(main_loss.item(), n)\n",
    "        aux_loss_meter.update(aux_loss.item(), n)\n",
    "        loss_meter.update(loss.item(), n)\n",
    "        \n",
    "        current_iter = epoch * len(train_loader) + i + 1\n",
    "        current_lr = poly_learning_rate(config.base_lr, current_iter, max_iter, power=config.power)\n",
    "        for index in range(0, index_split):\n",
    "            optimizer.param_groups[index]['lr'] = current_lr\n",
    "        for index in range(index_split, len(optimizer.param_groups)):\n",
    "            optimizer.param_groups[index]['lr'] = current_lr * 10\n",
    "            \n",
    "        \n",
    "        if (i+1) % print_freq == 1:\n",
    "            print(('Epoch: [{}/{}][{}/{}] '\n",
    "                        'MainLoss {main_loss_meter.val:.4f} '\n",
    "                        'AuxLoss {aux_loss_meter.val:.4f} '\n",
    "                        'Loss {loss_meter.val:.4f} '\n",
    "                        'Accuracy {accuracy:.4f}.'.format(epoch+1, config.num_epochs, i+1, len(train_loader),\n",
    "                                                          main_loss_meter=main_loss_meter,\n",
    "                                                          aux_loss_meter=aux_loss_meter,\n",
    "                                                          loss_meter=loss_meter,\n",
    "                                                          accuracy=accuracy)))\n",
    "\n",
    "    if (epoch) % val_every == 0:     \n",
    "        print('Start validation #{}'.format(epoch+1))\n",
    "        val_loss_meter, val_mIoU, val_mAcc, val_allAcc= validate(val_loader, model, criterion)\n",
    "        if val_mIoU > best_mIoU:\n",
    "            print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "            print('Save model in', saved_dir)\n",
    "            best_mIoU = val_mIoU\n",
    "            save_model(model, saved_dir)\n",
    "        \n",
    "#         wandb.log({\n",
    "#             'train_loss': main_loss_meter.val,\n",
    "#             'train_mIoU': aux_loss_meter,\n",
    "#             'train_mAcc': loss_meter,\n",
    "#             'train_allAcc': accuracy,\n",
    "#         })\n",
    "                \n",
    "    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n",
    "    mIoU = np.mean(iou_class)\n",
    "    mAcc = np.mean(accuracy_class)\n",
    "    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n",
    "    \n",
    "    return main_loss_meter.avg, mIoU, mAcc, allAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87dcc596-5719-4bfc-af6b-4c2c4cf0702f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion):\n",
    "    loss_meter = AverageMeter()\n",
    "    intersection_meter = AverageMeter()\n",
    "    union_meter = AverageMeter()\n",
    "    target_meter = AverageMeter()\n",
    "    \n",
    "    classes = 12\n",
    "    print_freq = 50\n",
    "    index_split = 0\n",
    "\n",
    "    model.eval()\n",
    "    for i, (input, target, _) in enumerate(val_loader):\n",
    "        \n",
    "        input = torch.stack(input)       # (batch, channel, height, width)\n",
    "        target = torch.stack(target).long()\n",
    "        \n",
    "        output = model(input)\n",
    "        if config.zoom_factor != 8:\n",
    "            output = F.interpolate(output, size=target.size()[1:], mode='bilinear', align_corners=True)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        n = input.size(0)\n",
    "        loss = torch.mean(loss)\n",
    "\n",
    "        output = output.max(1)[1]\n",
    "        intersection, union, target = intersectionAndUnionGPU(output, target, classes, config.ignore_label)\n",
    "        \n",
    "        intersection, union, target = intersection.cpu().numpy(), union.cpu().numpy(), target.cpu().numpy()\n",
    "        intersection_meter.update(intersection), union_meter.update(union), target_meter.update(target)\n",
    "\n",
    "        accuracy = sum(intersection_meter.val) / (sum(target_meter.val) + 1e-10)\n",
    "        loss_meter.update(loss.item(), input.size(0))\n",
    "\n",
    "    iou_class = intersection_meter.sum / (union_meter.sum + 1e-10)\n",
    "    accuracy_class = intersection_meter.sum / (target_meter.sum + 1e-10)\n",
    "    mIoU = np.mean(iou_class)\n",
    "    mAcc = np.mean(accuracy_class)\n",
    "    allAcc = sum(intersection_meter.sum) / (sum(target_meter.sum) + 1e-10)\n",
    "    \n",
    "    print('Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) '\n",
    "          'mIoU {mIoU:.4f} '\n",
    "          'Accuracy {accuracy:.4f}.'.format(loss_meter=loss_meter,\n",
    "                                            mIoU = mIoU,\n",
    "                                            accuracy=allAcc))\n",
    "    \n",
    "#     wandb.log({\n",
    "#         'val_loss': main_loss_meter.val,\n",
    "#         'val_mIoU': aux_loss_meter,\n",
    "#         'val_mAcc': loss_meter,\n",
    "#         'val_allAcc': allAcc,\n",
    "#     })\n",
    "        \n",
    "    return loss_meter.avg, mIoU, mAcc, allAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756ae51-5d4a-4655-b19e-c08727336e34",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb445839-5e6c-45c6-abb8-d8a50c1b2edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = '/opt/ml/code/saved'\n",
    "file_name = 'pspnet_best_model(pretrained).pt'\n",
    "\n",
    "if not os.path.isdir(saved_dir):                                                         \n",
    "    os.mkdir(saved_dir)\n",
    "    \n",
    "def save_model(model, saved_dir, file_name=file_name):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae5c985-12d2-4f04-a735-9f3cd78f0bae",
   "metadata": {},
   "source": [
    "## 모델 생성 및 Loss function, Optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680ae562-4ecd-41ba-a5e1-1b736be7bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function 정의\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=225)\n",
    "\n",
    "# 모델 생성\n",
    "classes = 12\n",
    "model = PSPNet(layers=50, classes=classes, zoom_factor=config.zoom_factor, criterion=criterion)\n",
    "\n",
    "# Optimizer 정의\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.base_lr, momentum=config.momentum, weight_decay=config.weight_decay)\n",
    "\n",
    "config.criterion = \"CrossEntropyLoss(ignore_index=225)\"\n",
    "config.optimizer = \"SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81092b-3340-4912-b1c4-a4e61c7018e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.alert(title=\"Done\", text=\"Train done.\")\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c55d05-4ab4-4eb3-968b-e9e4db66ad1d",
   "metadata": {},
   "source": [
    "## 저장된 model 불러오기 (학습된 이후) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a77497-bed0-4640-9a9f-13161be59b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = os.path.join('/opt/ml/code/saved', file_name)\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "model.eval()\n",
    "\n",
    "save_model = model\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8f74b-a75b-400d-8e45-95c035ab439c",
   "metadata": {},
   "source": [
    "## submission을 위한 test 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586badef-3bd2-473f-9133-efa510782114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(data_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            \n",
    "            if outs.shape[0] > 1:\n",
    "                oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            else:\n",
    "                oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f16389-c682-4f18-b803-f52dc9c9de74",
   "metadata": {},
   "source": [
    "## submission.csv 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374aeee-6e34-4abc-8f8e-0ddb63b00adf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae032159-8c07-4064-a59c-1bba0e85516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b4dba2-b8cd-4f58-84d3-cc18d0fa0260",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_test_loader = list(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2f892-5cf7-4a0e-8450-56e8a5df5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "print('type', type(lst_test_loader[i]), '| len', len(lst_test_loader[i]))\n",
    "print()\n",
    "\n",
    "print('type', type(lst_test_loader[i][0]), '| len', len(lst_test_loader[i][0]))\n",
    "\n",
    "for j in range(len(lst_test_loader[i][0])):\n",
    "    print(j,'- type', type(lst_test_loader[i][0][j]), '| shape', lst_test_loader[i][0][j].shape)\n",
    "print()\n",
    "\n",
    "print(lst_test_loader[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b2cb8-dbff-449d-8edd-6e0e6455e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 209\n",
    "print('type', type(lst_test_loader[i]), '| len', len(lst_test_loader[i]))\n",
    "print()\n",
    "\n",
    "print('type', type(lst_test_loader[i][0]), '| len', len(lst_test_loader[i][0]))\n",
    "\n",
    "for j in range(len(lst_test_loader[i][0])):\n",
    "    print(j,'- type', type(lst_test_loader[i][0][j]), '| shape', lst_test_loader[i][0][j].shape)\n",
    "print()\n",
    "\n",
    "print(lst_test_loader[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646aedd1-63c5-4755-a191-5fd1c8d8fca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫번째 batch의 추론 결과 확인\n",
    "for imgs, image_infos in test_loader:\n",
    "    image_infos = image_infos\n",
    "    temp_images = imgs\n",
    "    \n",
    "    model.eval()\n",
    "    # inference\n",
    "    outs = model(torch.stack(temp_images).to(device))\n",
    "    oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2096e-51c7-4686-9c24-55801768e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "imgs = lst_test_loader[i][0]\n",
    "\n",
    "outs = model(torch.stack(imgs).to(device))\n",
    "print('outs', outs.shape)\n",
    "\n",
    "oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "print('oms', oms.shape)\n",
    "\n",
    "for img, mask in zip(np.stack(temp_images), oms):\n",
    "    print(img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0634049e-a02d-4ab2-9e11-58f5f8f3ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "imgs = lst_test_loader[i][0]\n",
    "\n",
    "outs = model(torch.stack(imgs).to(device))\n",
    "print('outs', outs.shape)\n",
    "\n",
    "oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "print('oms', oms.shape)\n",
    "\n",
    "for img, mask in zip(np.stack(temp_images), oms):\n",
    "    print(img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868d3ee-8936-42f1-be5a-c7ee9d2ee179",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 209\n",
    "imgs = lst_test_loader[i][0]\n",
    "\n",
    "outs = model(torch.stack(imgs).to(device))\n",
    "print('outs', outs.shape[0])\n",
    "\n",
    "oms = torch.argmax(outs.squeeze(), dim=0).detach().cpu().numpy()\n",
    "print('oms', oms.shape)\n",
    "\n",
    "for img, mask in zip(np.stack(temp_images), oms):\n",
    "    print(img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b1dfb-d045-487d-bb32-6f7419fa98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 209\n",
    "imgs = lst_test_loader[i][0]\n",
    "\n",
    "outs = model(torch.stack(imgs).to(device))\n",
    "print('outs', outs.shape)\n",
    "\n",
    "oms = torch.argmax(outs, dim=1).detach().cpu().numpy()\n",
    "print('oms', oms.shape)\n",
    "\n",
    "for img, mask in zip(np.stack(temp_images), oms):\n",
    "    print(img.shape, mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826dd07-d474-4fab-b42d-276f2ba9aa26",
   "metadata": {
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d6ac5-2669-4cf7-8f3a-73ab45fccf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('/opt/ml/code/submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "file_name = 'PSPNet(pretrained).csv'\n",
    "submission.to_csv(os.path.join(\"/opt/ml/code/submission/\", file_name), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ac119-b7ab-460e-a0aa-b59146d44ad1",
   "metadata": {},
   "source": [
    "## Check model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684c434-44d5-4fe8-8223-a3979f72f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27d1644-b826-4578-a922-a21240ee6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5e7f1-497f-40a6-b652-a9fc3b1b13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.layer4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ce740-703e-4268-8b40-abc18d836aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.conv2, resnet.bn2, resnet.relu, resnet.conv3, resnet.bn3, resnet.relu, resnet.maxpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5efda99-5a1f-417d-9c4d-b01ec9c069b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
