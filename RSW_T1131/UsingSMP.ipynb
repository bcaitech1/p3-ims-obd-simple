{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f3c23a-0a51-4cff-a38d-8fc92b3d998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-PCIE-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfca55e-97f9-42a4-81e0-bbe092032336",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터, Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7dcd7c-eabc-41bb-bbc7-a3062765fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "class cfg:\n",
    "    batch_size = 16\n",
    "    num_epochs = 20\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-6\n",
    "    model_name = \"unet\"\n",
    "    encoders = \"efficientnet-b4\"\n",
    "    encode_weights = \"imagenet\"\n",
    "\n",
    "# data_path\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c8e765-610c-4e60-97a5-4e0699275a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7a02a3-38ad-4a50-8538-20b7c9eecd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Backgroud',\n",
    " 'UNKNOWN',\n",
    " 'General trash',\n",
    " 'Paper',\n",
    " 'Paper pack',\n",
    " 'Metal',\n",
    " 'Glass',\n",
    " 'Plastic',\n",
    " 'Styrofoam',\n",
    " 'Plastic bag',\n",
    " 'Battery',\n",
    " 'Clothing']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8c62a-f898-4bd5-8c1e-acae423b4d76",
   "metadata": {},
   "source": [
    "# Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b2bbd72-e4a8-4c82-83c9-a591df036160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115e8b5-0160-4228-9551-388a6b32f4dc",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df6ca809-0881-4456-90a0-4f2aca69a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.98s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_transform = A.Compose([\n",
    "                            A.Resize(256, 256),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "                          A.Resize(256, 256),\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=cfg.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=cfg.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=cfg.batch_size,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8b90b-6088-45d8-b389-8ad63273f637",
   "metadata": {},
   "source": [
    "# train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc2ccb5-096c-4319-9133-37bbd6375443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print('Start training..')\n",
    "    best_loss = 9999999\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "                  \n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # loss 계산 (cross entropy loss)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n",
    "        \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32e4f08-4428-48a0-bb4b-fdf44c24f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        mIoU_list = []\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            \n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)            \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "\n",
    "            mIoU = label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)[2]\n",
    "            mIoU_list.append(mIoU)\n",
    "            \n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}'.format(epoch, avrg_loss, np.mean(mIoU_list)))\n",
    "\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6181b98-5203-4043-8bec-ef9b48bac096",
   "metadata": {},
   "source": [
    "# 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4919ffcb-163b-49e6-953d-96ec029bfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1 \n",
    "\n",
    "saved_dir = './saved'\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "    \n",
    "def save_model(model, saved_dir, file_name=f'{cfg.model_name}_best_model.pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76901c-0d5d-4400-9f76-77a00e29c826",
   "metadata": {},
   "source": [
    "# model, loss, optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aa8465a-bdc9-4c01-9d04-e78891bdd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([1, 3, 512, 512])\n",
      "output shape :  torch.Size([1, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=cfg.encoders,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=cfg.encode_weights,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=12,  \n",
    ")\n",
    "#num_classes=12\n",
    "x = torch.randn([1, 3, 512, 512])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x).to(device)\n",
    "print(\"output shape : \", out.size())\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer 정의\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = cfg.learning_rate, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8df1a-a54f-4e28-8bf1-4508b7275b23",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea72e7c7-ecf8-4d69-bef3-f263ceff116e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/20], Step [25/164], Loss: 2.5621\n",
      "Epoch [1/20], Step [50/164], Loss: 2.2239\n",
      "Epoch [1/20], Step [75/164], Loss: 1.9184\n",
      "Epoch [1/20], Step [100/164], Loss: 1.6758\n",
      "Epoch [1/20], Step [125/164], Loss: 1.4678\n",
      "Epoch [1/20], Step [150/164], Loss: 1.2541\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 1.1734, mIoU: 0.1600\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved\n",
      "Epoch [2/20], Step [25/164], Loss: 0.9610\n",
      "Epoch [2/20], Step [50/164], Loss: 1.0228\n",
      "Epoch [2/20], Step [75/164], Loss: 0.8837\n",
      "Epoch [2/20], Step [100/164], Loss: 0.7668\n",
      "Epoch [2/20], Step [125/164], Loss: 0.7161\n",
      "Epoch [2/20], Step [150/164], Loss: 0.9433\n",
      "Start validation #2\n",
      "Validation #2  Average Loss: 0.6812, mIoU: 0.1838\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved\n",
      "Epoch [3/20], Step [25/164], Loss: 0.6437\n",
      "Epoch [3/20], Step [50/164], Loss: 0.5563\n",
      "Epoch [3/20], Step [75/164], Loss: 0.6008\n",
      "Epoch [3/20], Step [100/164], Loss: 0.4478\n",
      "Epoch [3/20], Step [125/164], Loss: 0.5471\n",
      "Epoch [3/20], Step [150/164], Loss: 0.6423\n",
      "Start validation #3\n",
      "Validation #3  Average Loss: 0.5360, mIoU: 0.2247\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved\n",
      "Epoch [4/20], Step [25/164], Loss: 0.4594\n",
      "Epoch [4/20], Step [50/164], Loss: 0.4444\n",
      "Epoch [4/20], Step [75/164], Loss: 0.4815\n",
      "Epoch [4/20], Step [100/164], Loss: 0.4152\n",
      "Epoch [4/20], Step [125/164], Loss: 0.4025\n",
      "Epoch [4/20], Step [150/164], Loss: 0.3066\n",
      "Start validation #4\n",
      "Validation #4  Average Loss: 0.4777, mIoU: 0.2558\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved\n",
      "Epoch [5/20], Step [25/164], Loss: 0.5297\n",
      "Epoch [5/20], Step [50/164], Loss: 0.5462\n",
      "Epoch [5/20], Step [75/164], Loss: 0.5208\n",
      "Epoch [5/20], Step [100/164], Loss: 0.3757\n",
      "Epoch [5/20], Step [125/164], Loss: 0.2544\n",
      "Epoch [5/20], Step [150/164], Loss: 0.4048\n",
      "Start validation #5\n",
      "Validation #5  Average Loss: 0.4410, mIoU: 0.2773\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved\n",
      "Epoch [6/20], Step [25/164], Loss: 0.3090\n",
      "Epoch [6/20], Step [50/164], Loss: 0.4844\n",
      "Epoch [6/20], Step [75/164], Loss: 0.4562\n",
      "Epoch [6/20], Step [100/164], Loss: 0.4031\n",
      "Epoch [6/20], Step [125/164], Loss: 0.3005\n",
      "Epoch [6/20], Step [150/164], Loss: 0.5372\n",
      "Start validation #6\n",
      "Validation #6  Average Loss: 0.4240, mIoU: 0.3099\n",
      "Best performance at epoch: 6\n",
      "Save model in ./saved\n",
      "Epoch [7/20], Step [25/164], Loss: 0.3260\n",
      "Epoch [7/20], Step [50/164], Loss: 0.2202\n",
      "Epoch [7/20], Step [75/164], Loss: 0.4078\n",
      "Epoch [7/20], Step [100/164], Loss: 0.3186\n",
      "Epoch [7/20], Step [125/164], Loss: 0.2797\n",
      "Epoch [7/20], Step [150/164], Loss: 0.2638\n",
      "Start validation #7\n",
      "Validation #7  Average Loss: 0.4169, mIoU: 0.3182\n",
      "Best performance at epoch: 7\n",
      "Save model in ./saved\n",
      "Epoch [8/20], Step [25/164], Loss: 0.2359\n",
      "Epoch [8/20], Step [50/164], Loss: 0.1950\n",
      "Epoch [8/20], Step [75/164], Loss: 0.3157\n",
      "Epoch [8/20], Step [100/164], Loss: 0.2713\n",
      "Epoch [8/20], Step [125/164], Loss: 0.3181\n",
      "Epoch [8/20], Step [150/164], Loss: 0.2695\n",
      "Start validation #8\n",
      "Validation #8  Average Loss: 0.4037, mIoU: 0.3215\n",
      "Best performance at epoch: 8\n",
      "Save model in ./saved\n",
      "Epoch [9/20], Step [25/164], Loss: 0.2497\n",
      "Epoch [9/20], Step [50/164], Loss: 0.1871\n",
      "Epoch [9/20], Step [75/164], Loss: 0.2570\n",
      "Epoch [9/20], Step [100/164], Loss: 0.2454\n",
      "Epoch [9/20], Step [125/164], Loss: 0.2630\n",
      "Epoch [9/20], Step [150/164], Loss: 0.2417\n",
      "Start validation #9\n",
      "Validation #9  Average Loss: 0.4009, mIoU: 0.3197\n",
      "Best performance at epoch: 9\n",
      "Save model in ./saved\n",
      "Epoch [10/20], Step [25/164], Loss: 0.2189\n",
      "Epoch [10/20], Step [50/164], Loss: 0.1334\n",
      "Epoch [10/20], Step [75/164], Loss: 0.2085\n",
      "Epoch [10/20], Step [100/164], Loss: 0.2407\n",
      "Epoch [10/20], Step [125/164], Loss: 0.1855\n",
      "Epoch [10/20], Step [150/164], Loss: 0.1826\n",
      "Start validation #10\n",
      "Validation #10  Average Loss: 0.4113, mIoU: 0.3120\n",
      "Epoch [11/20], Step [25/164], Loss: 0.1486\n",
      "Epoch [11/20], Step [50/164], Loss: 0.3236\n",
      "Epoch [11/20], Step [75/164], Loss: 0.2500\n",
      "Epoch [11/20], Step [100/164], Loss: 0.1719\n",
      "Epoch [11/20], Step [125/164], Loss: 0.1965\n",
      "Epoch [11/20], Step [150/164], Loss: 0.2199\n",
      "Start validation #11\n",
      "Validation #11  Average Loss: 0.3916, mIoU: 0.3142\n",
      "Best performance at epoch: 11\n",
      "Save model in ./saved\n",
      "Epoch [12/20], Step [25/164], Loss: 0.1405\n",
      "Epoch [12/20], Step [50/164], Loss: 0.2245\n",
      "Epoch [12/20], Step [75/164], Loss: 0.1747\n",
      "Epoch [12/20], Step [100/164], Loss: 0.1470\n",
      "Epoch [12/20], Step [125/164], Loss: 0.1831\n",
      "Epoch [12/20], Step [150/164], Loss: 0.2542\n",
      "Start validation #12\n",
      "Validation #12  Average Loss: 0.3993, mIoU: 0.3135\n",
      "Epoch [13/20], Step [25/164], Loss: 0.2322\n",
      "Epoch [13/20], Step [50/164], Loss: 0.1578\n",
      "Epoch [13/20], Step [75/164], Loss: 0.1185\n",
      "Epoch [13/20], Step [100/164], Loss: 0.1444\n",
      "Epoch [13/20], Step [125/164], Loss: 0.1746\n",
      "Epoch [13/20], Step [150/164], Loss: 0.1205\n",
      "Start validation #13\n",
      "Validation #13  Average Loss: 0.4022, mIoU: 0.3136\n",
      "Epoch [14/20], Step [25/164], Loss: 0.2322\n",
      "Epoch [14/20], Step [50/164], Loss: 0.1514\n",
      "Epoch [14/20], Step [75/164], Loss: 0.1609\n",
      "Epoch [14/20], Step [100/164], Loss: 0.1734\n",
      "Epoch [14/20], Step [125/164], Loss: 0.1542\n",
      "Epoch [14/20], Step [150/164], Loss: 0.1560\n",
      "Start validation #14\n",
      "Validation #14  Average Loss: 0.4010, mIoU: 0.3233\n",
      "Epoch [15/20], Step [25/164], Loss: 0.1208\n",
      "Epoch [15/20], Step [50/164], Loss: 0.2448\n",
      "Epoch [15/20], Step [75/164], Loss: 0.1629\n",
      "Epoch [15/20], Step [100/164], Loss: 0.1110\n",
      "Epoch [15/20], Step [125/164], Loss: 0.2258\n",
      "Epoch [15/20], Step [150/164], Loss: 0.1584\n",
      "Start validation #15\n",
      "Validation #15  Average Loss: 0.4071, mIoU: 0.3273\n",
      "Epoch [16/20], Step [25/164], Loss: 0.1671\n",
      "Epoch [16/20], Step [50/164], Loss: 0.2294\n",
      "Epoch [16/20], Step [75/164], Loss: 0.1300\n",
      "Epoch [16/20], Step [100/164], Loss: 0.1831\n",
      "Epoch [16/20], Step [125/164], Loss: 0.1557\n",
      "Epoch [16/20], Step [150/164], Loss: 0.2256\n",
      "Start validation #16\n",
      "Validation #16  Average Loss: 0.4029, mIoU: 0.3337\n",
      "Epoch [17/20], Step [25/164], Loss: 0.1478\n",
      "Epoch [17/20], Step [50/164], Loss: 0.1604\n",
      "Epoch [17/20], Step [75/164], Loss: 0.1687\n",
      "Epoch [17/20], Step [100/164], Loss: 0.1475\n",
      "Epoch [17/20], Step [125/164], Loss: 0.1413\n",
      "Epoch [17/20], Step [150/164], Loss: 0.1524\n",
      "Start validation #17\n",
      "Validation #17  Average Loss: 0.4011, mIoU: 0.3365\n",
      "Epoch [18/20], Step [25/164], Loss: 0.1393\n",
      "Epoch [18/20], Step [50/164], Loss: 0.1190\n",
      "Epoch [18/20], Step [75/164], Loss: 0.1141\n",
      "Epoch [18/20], Step [100/164], Loss: 0.0860\n",
      "Epoch [18/20], Step [125/164], Loss: 0.1389\n",
      "Epoch [18/20], Step [150/164], Loss: 0.1147\n",
      "Start validation #18\n",
      "Validation #18  Average Loss: 0.4373, mIoU: 0.3373\n",
      "Epoch [19/20], Step [25/164], Loss: 0.1587\n",
      "Epoch [19/20], Step [50/164], Loss: 0.1846\n",
      "Epoch [19/20], Step [75/164], Loss: 0.1490\n",
      "Epoch [19/20], Step [100/164], Loss: 0.1332\n",
      "Epoch [19/20], Step [125/164], Loss: 0.0904\n",
      "Epoch [19/20], Step [150/164], Loss: 0.1214\n",
      "Start validation #19\n",
      "Validation #19  Average Loss: 0.4127, mIoU: 0.3455\n",
      "Epoch [20/20], Step [25/164], Loss: 0.1019\n",
      "Epoch [20/20], Step [50/164], Loss: 0.1145\n",
      "Epoch [20/20], Step [75/164], Loss: 0.1356\n",
      "Epoch [20/20], Step [100/164], Loss: 0.1038\n",
      "Epoch [20/20], Step [125/164], Loss: 0.1504\n",
      "Epoch [20/20], Step [150/164], Loss: 0.0811\n",
      "Start validation #20\n",
      "Validation #20  Average Loss: 0.4060, mIoU: 0.3520\n"
     ]
    }
   ],
   "source": [
    "train(cfg.num_epochs, model, train_loader, val_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d114be-9aa8-468e-a978-fabf1e679acd",
   "metadata": {},
   "source": [
    "# load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e874c493-716f-423d-8593-625e06ff993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = f'./saved/unet_best_model.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4002e-65ce-42cb-bc09-1f79403d4fcd",
   "metadata": {},
   "source": [
    "# test for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "020b1ea8-e7bf-480a-a9c5-d520ac1390dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6cd5da-d6a9-4d0c-b5fd-b1a802283637",
   "metadata": {},
   "source": [
    "# submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcab6099-70e3-4933-a858-e604d8e407c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction.\n",
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(f\"./submission/{cfg.model_name}_{cfg.encoders}_{cfg.encode_weights}(pretrained).csv\", index=False)f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b66077-a63c-4846-9205-3129d5206dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
