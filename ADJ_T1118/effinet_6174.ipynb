{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd87019-b7c5-4cdc-82e4-fc96d38e9388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f3c23a-0a51-4cff-a38d-8fc92b3d998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n",
      "Tesla V100-PCIE-32GB\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import label_accuracy_score\n",
    "\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리를 위한 라이브러리\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchgeometry as tgm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# 시각화를 위한 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfca55e-97f9-42a4-81e0-bbe092032336",
   "metadata": {},
   "source": [
    "# 하이퍼파라미터, Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7dcd7c-eabc-41bb-bbc7-a3062765fd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "class cfg:\n",
    "    batch_size = 16\n",
    "    num_epochs = 40\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-6\n",
    "    model_name = \"DeepLabV3Plus\"\n",
    "    encoders = \"efficientnet-b4\"\n",
    "    encode_weights = \"imagenet\"\n",
    "\n",
    "# data_path\n",
    "dataset_path = '../input/data'\n",
    "anns_file_path = dataset_path + '/' + 'train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4b27ca-b8e7-4462-a0f5-d200242b9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# albumentations.augmentations.transforms.HueSaturationValue (hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, always_apply=False, p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c8e765-610c-4e60-97a5-4e0699275a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed 고정\n",
    "random_seed = 21\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.dete2rministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7a02a3-38ad-4a50-8538-20b7c9eecd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = ['Backgroud',\n",
    " 'UNKNOWN',\n",
    " 'General trash',\n",
    " 'Paper',\n",
    " 'Paper pack',\n",
    " 'Metal',\n",
    " 'Glass',\n",
    " 'Plastic',\n",
    " 'Styrofoam',\n",
    " 'Plastic bag',\n",
    " 'Battery',\n",
    " 'Clothing']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1dcc5-c49d-4068-9cea-9f84bbd1b347",
   "metadata": {},
   "source": [
    "# Loss 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a42f142c-94d9-4074-91c7-c1cf49822724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.focal = tgm.losses.FocalLoss(alpha=0.5, gamma=2.0, reduction='mean')\n",
    "        self.dice = tgm.losses.DiceLoss()\n",
    "\n",
    "    def forward(self,input_tensor,target_tensor):\n",
    "        focal_loss = self.focal(input_tensor,target_tensor)\n",
    "        dice_loss = self.dice(input_tensor, target_tensor)\n",
    "        \n",
    "        loss = focal_loss + dice_loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8c62a-f898-4bd5-8c1e-acae423b4d76",
   "metadata": {},
   "source": [
    "# Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2bbd72-e4a8-4c82-83c9-a591df036160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.float32)\n",
    "\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115e8b5-0160-4228-9551-388a6b32f4dc",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df6ca809-0881-4456-90a0-4f2aca69a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.15s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=4.35s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=1.36s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# train.json / validation.json / test.json 디렉토리 설정\n",
    "train_path = dataset_path + '/train.json'\n",
    "val_path = dataset_path + '/val.json'\n",
    "test_path = dataset_path + '/test.json'\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "#                             A.Resize(256,256),\n",
    "#                             A.HorizontalFlip(p=1.0),\n",
    "#                             A.CropNonEmptyMaskIfExists(256,256,p=0.5),\n",
    "#                             A.RandomRotate90(p=0.5),\n",
    "# #                             A.ElasticTransform(p=0.5),\n",
    "#                             A.GridDistortion(p=0.5),\n",
    "# #                             A.OpticalDistortion(p=0.5),\n",
    "# #                             A.ShiftScaleRotate(p=0.5),\n",
    "train_transform = A.Compose([\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.Rotate(limit=90),\n",
    "                            A.transforms.ColorJitter(),\n",
    "                            #A.OneOf([\n",
    "                            #    A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                            #    A.GridDistortion(p=0.5),\n",
    "                            #    A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "                            #], p=0.5),\n",
    "                            #A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            #  std=[0.229, 0.224, 0.225]),\n",
    "                            ToTensorV2()\n",
    "                            \n",
    "                            ])\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "val_transform = A.Compose([\n",
    "                          A.HorizontalFlip(p=0.5),\n",
    "                          A.Rotate(limit=90),\n",
    "                          A.transforms.ColorJitter(),\n",
    "                          #A.OneOf([\n",
    "                          #      A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                          #      A.GridDistortion(p=0.5),\n",
    "                          #      A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "                          #  ], p=0.5),  \n",
    "                          #A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          #    std=[0.229, 0.224, 0.225]),\n",
    "                          ToTensorV2()\n",
    "                          ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            \n",
    "                           ToTensorV2()\n",
    "                           ])\n",
    "\n",
    "# create own Dataset 1 (skip)\n",
    "# validation set을 직접 나누고 싶은 경우\n",
    "# random_split 사용하여 data set을 8:2 로 분할\n",
    "# train_size = int(0.8*len(dataset))\n",
    "# val_size = int(len(dataset)-train_size)\n",
    "# dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=transform)\n",
    "# train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# create own Dataset 2\n",
    "# train dataset\n",
    "train_dataset = CustomDataLoader(data_dir=train_path, mode='train', transform=train_transform)\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = CustomDataLoader(data_dir=val_path, mode='val', transform=val_transform)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = CustomDataLoader(data_dir=test_path, mode='test', transform=test_transform)\n",
    "\n",
    "\n",
    "# DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=cfg.batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                         batch_size=cfg.batch_size,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4,\n",
    "                                         collate_fn=collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=16,\n",
    "                                          num_workers=4,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02476e8-c9cb-47cf-8b74-bd847b69dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingWarmUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5480e56f-b114-484e-9612-c15d59646fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f763456-7afb-4a88-93a7-8559808c7eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58162fdc-6ce7-4ee1-99a4-15129237f564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a8b90b-6088-45d8-b389-8ad63273f637",
   "metadata": {},
   "source": [
    "# train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fc2ccb5-096c-4319-9133-37bbd6375443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, val_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print('Start training..')\n",
    "    best_loss = 9999999\n",
    "    best_epoch = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        cnt = 0\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "            \n",
    "            # gpu 연산을 위해 device 할당\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # inference\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            # step 주기에 따른 loss 출력\n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n",
    "                total_loss += loss.item()\n",
    "                cnt += 1\n",
    "        print(f'Train Average Loss:{(total_loss/cnt):.4f}')\n",
    "        \n",
    "        # validation 주기에 따른 loss 출력 및 best model 저장\n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = validation(epoch + 1, model, val_loader, criterion, device)\n",
    "            if avrg_loss < best_loss:\n",
    "                best_epoch = epoch + 1\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c32e4f08-4428-48a0-bb4b-fdf44c24f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        mIoU_list = []\n",
    "        for step, (images, masks, _) in enumerate(data_loader):\n",
    "            \n",
    "            images = torch.stack(images)       # (batch, channel, height, width)\n",
    "            masks = torch.stack(masks).long()  # (batch, channel, height, width)\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)            \n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "            \n",
    "            outputs = torch.argmax(outputs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "\n",
    "            mIoU = label_accuracy_score(masks.detach().cpu().numpy(), outputs, n_class=12)[2]\n",
    "            mIoU_list.append(mIoU)\n",
    "            \n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Average Loss: {:.4f}, mIoU: {:.4f}'.format(epoch, avrg_loss, np.mean(mIoU_list)))\n",
    "\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6181b98-5203-4043-8bec-ef9b48bac096",
   "metadata": {},
   "source": [
    "# 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4919ffcb-163b-49e6-953d-96ec029bfd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 함수 정의\n",
    "val_every = 1 \n",
    "\n",
    "saved_dir = './saved'\n",
    "if not os.path.isdir(saved_dir):                                                           \n",
    "    os.mkdir(saved_dir)\n",
    "    \n",
    "def save_model(model, saved_dir, file_name=f'{cfg.model_name}_{cfg.encoders}_{cfg.encode_weights}_best_model.pt'):\n",
    "    check_point = {'net': model.state_dict()}\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model.state_dict(), output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76901c-0d5d-4400-9f76-77a00e29c826",
   "metadata": {},
   "source": [
    "# model, loss, optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09dcc25e-0e9b-4123-8245-88412844f2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May  4 19:52:39 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.56       Driver Version: 418.56       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:05.0 Off |                  Off |\n",
      "| N/A   39C    P0    26W / 250W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9312e04a-8ef4-4033-bb87-85e15a4201a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     USER        PID ACCESS COMMAND\n",
      "/dev/nvidia-uvm:     root     kernel mount /dev/nvidia-uvm\n",
      "                     root      19298 F.... python\n",
      "/dev/nvidia-uvm-tools:\n",
      "                     root     kernel mount /dev/nvidia-uvm-tools\n",
      "/dev/nvidia0:        root     kernel mount /dev/nvidia0\n",
      "                     root      19298 F...m python\n",
      "/dev/nvidiactl:      root     kernel mount /dev/nvidiactl\n",
      "                     root      19298 F.... python\n"
     ]
    }
   ],
   "source": [
    "!fuser -v /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ead46-d82b-4466-8415-42f40896bbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa8465a-bdc9-4c01-9d04-e78891bdd5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model name: DeepLabV3Plus\n",
      "input shape :  torch.Size([2, 3, 512, 512])\n",
      "output shape :  torch.Size([2, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "if cfg.model_name in [\"DeepLabV3Plus\", \"deeplabv3plus\"]:\n",
    "    model = smp.DeepLabV3Plus(encoder_name=cfg.encoders,\n",
    "                              encoder_weights=cfg.encode_weights,\n",
    "                              in_channels=3,\n",
    "                              classes=12)\n",
    "    \n",
    "# elif cfg.model_name in [\"DeepLabV3Plus\",\"deeplabv3plus\"]:\n",
    "#     model = smp.DeepLabV3Plus(\n",
    "#         encoder_name=cfg.encoders,        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "#         encoder_weights=cfg.encode_weights,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "#         in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "#         classes=12,  \n",
    "#     )\n",
    "\n",
    "\n",
    "x = torch.randn([2, 3, 512, 512])\n",
    "print(f'model name: {cfg.model_name}')\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x).to(device)\n",
    "print(\"output shape : \", out.size())\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "focal = tgm.losses.FocalLoss(alpha=0.5, gamma=2.0, reduction='mean')\n",
    "dice = tgm.losses.DiceLoss()\n",
    "\n",
    "custom = CustomLoss()\n",
    "    \n",
    "# Optimizer 정의\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = cfg.learning_rate, weight_decay=cfg.weight_decay)\n",
    "scheduler = CosineAnnealingWarmUpRestarts(optimizer,T_mult=1,eta_max=1e-4,T_0=20,T_up=2,gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f8df1a-a54f-4e28-8bf1-4508b7275b23",
   "metadata": {},
   "source": [
    "# Train Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8cbdc78-56de-496b-b6c3-d8adae53ba88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch [1/40], Step [25/164], Loss: 1.2824\n",
      "Epoch [1/40], Step [50/164], Loss: 1.0776\n",
      "Epoch [1/40], Step [75/164], Loss: 0.7593\n",
      "Epoch [1/40], Step [100/164], Loss: 0.7442\n",
      "Epoch [1/40], Step [125/164], Loss: 0.6196\n",
      "Epoch [1/40], Step [150/164], Loss: 0.5938\n",
      "Train Average Loss:0.8462\n",
      "Start validation #1\n",
      "Validation #1  Average Loss: 0.5728, mIoU: 0.2335\n",
      "Best performance at epoch: 1\n",
      "Save model in ./saved\n",
      "Epoch [2/40], Step [25/164], Loss: 0.5758\n",
      "Epoch [2/40], Step [50/164], Loss: 0.5069\n",
      "Epoch [2/40], Step [75/164], Loss: 0.5049\n",
      "Epoch [2/40], Step [100/164], Loss: 0.4307\n",
      "Epoch [2/40], Step [125/164], Loss: 0.4447\n",
      "Epoch [2/40], Step [150/164], Loss: 0.5341\n",
      "Train Average Loss:0.4995\n",
      "Start validation #2\n",
      "Validation #2  Average Loss: 0.4106, mIoU: 0.2996\n",
      "Best performance at epoch: 2\n",
      "Save model in ./saved\n",
      "Epoch [3/40], Step [25/164], Loss: 0.4676\n",
      "Epoch [3/40], Step [50/164], Loss: 0.3666\n",
      "Epoch [3/40], Step [75/164], Loss: 0.3395\n",
      "Epoch [3/40], Step [100/164], Loss: 0.4139\n",
      "Epoch [3/40], Step [125/164], Loss: 0.3222\n",
      "Epoch [3/40], Step [150/164], Loss: 0.5201\n",
      "Train Average Loss:0.4050\n",
      "Start validation #3\n",
      "Validation #3  Average Loss: 0.3311, mIoU: 0.3329\n",
      "Best performance at epoch: 3\n",
      "Save model in ./saved\n",
      "Epoch [4/40], Step [25/164], Loss: 0.2810\n",
      "Epoch [4/40], Step [50/164], Loss: 0.3469\n",
      "Epoch [4/40], Step [75/164], Loss: 0.2779\n",
      "Epoch [4/40], Step [100/164], Loss: 0.2710\n",
      "Epoch [4/40], Step [125/164], Loss: 0.3136\n",
      "Epoch [4/40], Step [150/164], Loss: 0.2949\n",
      "Train Average Loss:0.2975\n",
      "Start validation #4\n",
      "Validation #4  Average Loss: 0.2956, mIoU: 0.3688\n",
      "Best performance at epoch: 4\n",
      "Save model in ./saved\n",
      "Epoch [5/40], Step [25/164], Loss: 0.3594\n",
      "Epoch [5/40], Step [50/164], Loss: 0.3902\n",
      "Epoch [5/40], Step [75/164], Loss: 0.2330\n",
      "Epoch [5/40], Step [100/164], Loss: 0.3079\n",
      "Epoch [5/40], Step [125/164], Loss: 0.2955\n",
      "Epoch [5/40], Step [150/164], Loss: 0.1734\n",
      "Train Average Loss:0.2932\n",
      "Start validation #5\n",
      "Validation #5  Average Loss: 0.2603, mIoU: 0.3931\n",
      "Best performance at epoch: 5\n",
      "Save model in ./saved\n",
      "Epoch [6/40], Step [25/164], Loss: 0.2928\n",
      "Epoch [6/40], Step [50/164], Loss: 0.3904\n",
      "Epoch [6/40], Step [75/164], Loss: 0.1766\n",
      "Epoch [6/40], Step [100/164], Loss: 0.2306\n",
      "Epoch [6/40], Step [125/164], Loss: 0.2634\n",
      "Epoch [6/40], Step [150/164], Loss: 0.2079\n",
      "Train Average Loss:0.2603\n",
      "Start validation #6\n",
      "Validation #6  Average Loss: 0.2464, mIoU: 0.4023\n",
      "Best performance at epoch: 6\n",
      "Save model in ./saved\n",
      "Epoch [7/40], Step [25/164], Loss: 0.2727\n",
      "Epoch [7/40], Step [50/164], Loss: 0.2167\n",
      "Epoch [7/40], Step [75/164], Loss: 0.1952\n",
      "Epoch [7/40], Step [100/164], Loss: 0.2255\n",
      "Epoch [7/40], Step [125/164], Loss: 0.2693\n",
      "Epoch [7/40], Step [150/164], Loss: 0.2391\n",
      "Train Average Loss:0.2364\n",
      "Start validation #7\n",
      "Validation #7  Average Loss: 0.2373, mIoU: 0.4122\n",
      "Best performance at epoch: 7\n",
      "Save model in ./saved\n",
      "Epoch [8/40], Step [25/164], Loss: 0.1380\n",
      "Epoch [8/40], Step [50/164], Loss: 0.3309\n",
      "Epoch [8/40], Step [75/164], Loss: 0.1951\n",
      "Epoch [8/40], Step [100/164], Loss: 0.2933\n",
      "Epoch [8/40], Step [125/164], Loss: 0.2050\n",
      "Epoch [8/40], Step [150/164], Loss: 0.2141\n",
      "Train Average Loss:0.2294\n",
      "Start validation #8\n",
      "Validation #8  Average Loss: 0.2244, mIoU: 0.4346\n",
      "Best performance at epoch: 8\n",
      "Save model in ./saved\n",
      "Epoch [9/40], Step [25/164], Loss: 0.1835\n",
      "Epoch [9/40], Step [50/164], Loss: 0.2198\n",
      "Epoch [9/40], Step [75/164], Loss: 0.3125\n",
      "Epoch [9/40], Step [100/164], Loss: 0.2266\n",
      "Epoch [9/40], Step [125/164], Loss: 0.2080\n",
      "Epoch [9/40], Step [150/164], Loss: 0.2226\n",
      "Train Average Loss:0.2288\n",
      "Start validation #9\n",
      "Validation #9  Average Loss: 0.2156, mIoU: 0.4323\n",
      "Best performance at epoch: 9\n",
      "Save model in ./saved\n",
      "Epoch [10/40], Step [25/164], Loss: 0.1563\n",
      "Epoch [10/40], Step [50/164], Loss: 0.1676\n",
      "Epoch [10/40], Step [75/164], Loss: 0.2656\n",
      "Epoch [10/40], Step [100/164], Loss: 0.1801\n",
      "Epoch [10/40], Step [125/164], Loss: 0.1520\n",
      "Epoch [10/40], Step [150/164], Loss: 0.2222\n",
      "Train Average Loss:0.1906\n",
      "Start validation #10\n",
      "Validation #10  Average Loss: 0.2160, mIoU: 0.4378\n",
      "Epoch [11/40], Step [25/164], Loss: 0.1929\n",
      "Epoch [11/40], Step [50/164], Loss: 0.1704\n",
      "Epoch [11/40], Step [75/164], Loss: 0.1825\n",
      "Epoch [11/40], Step [100/164], Loss: 0.1355\n",
      "Epoch [11/40], Step [125/164], Loss: 0.1399\n",
      "Epoch [11/40], Step [150/164], Loss: 0.1300\n",
      "Train Average Loss:0.1585\n",
      "Start validation #11\n",
      "Validation #11  Average Loss: 0.2114, mIoU: 0.4372\n",
      "Best performance at epoch: 11\n",
      "Save model in ./saved\n",
      "Epoch [12/40], Step [25/164], Loss: 0.1693\n",
      "Epoch [12/40], Step [50/164], Loss: 0.1719\n",
      "Epoch [12/40], Step [75/164], Loss: 0.1193\n",
      "Epoch [12/40], Step [100/164], Loss: 0.1717\n",
      "Epoch [12/40], Step [125/164], Loss: 0.1283\n",
      "Epoch [12/40], Step [150/164], Loss: 0.1503\n",
      "Train Average Loss:0.1518\n",
      "Start validation #12\n",
      "Validation #12  Average Loss: 0.2102, mIoU: 0.4444\n",
      "Best performance at epoch: 12\n",
      "Save model in ./saved\n",
      "Epoch [13/40], Step [25/164], Loss: 0.1145\n",
      "Epoch [13/40], Step [50/164], Loss: 0.1425\n",
      "Epoch [13/40], Step [75/164], Loss: 0.1418\n",
      "Epoch [13/40], Step [100/164], Loss: 0.1854\n",
      "Epoch [13/40], Step [125/164], Loss: 0.1814\n",
      "Epoch [13/40], Step [150/164], Loss: 0.0934\n",
      "Train Average Loss:0.1432\n",
      "Start validation #13\n",
      "Validation #13  Average Loss: 0.2070, mIoU: 0.4466\n",
      "Best performance at epoch: 13\n",
      "Save model in ./saved\n",
      "Epoch [14/40], Step [25/164], Loss: 0.1098\n",
      "Epoch [14/40], Step [50/164], Loss: 0.1109\n",
      "Epoch [14/40], Step [75/164], Loss: 0.1353\n",
      "Epoch [14/40], Step [100/164], Loss: 0.2206\n",
      "Epoch [14/40], Step [125/164], Loss: 0.1113\n",
      "Epoch [14/40], Step [150/164], Loss: 0.1316\n",
      "Train Average Loss:0.1366\n",
      "Start validation #14\n",
      "Validation #14  Average Loss: 0.2060, mIoU: 0.4568\n",
      "Best performance at epoch: 14\n",
      "Save model in ./saved\n",
      "Epoch [15/40], Step [25/164], Loss: 0.1026\n",
      "Epoch [15/40], Step [50/164], Loss: 0.1828\n",
      "Epoch [15/40], Step [75/164], Loss: 0.1007\n",
      "Epoch [15/40], Step [100/164], Loss: 0.1715\n",
      "Epoch [15/40], Step [125/164], Loss: 0.1009\n",
      "Epoch [15/40], Step [150/164], Loss: 0.1348\n",
      "Train Average Loss:0.1322\n",
      "Start validation #15\n",
      "Validation #15  Average Loss: 0.2093, mIoU: 0.4485\n",
      "Epoch [16/40], Step [25/164], Loss: 0.1014\n",
      "Epoch [16/40], Step [50/164], Loss: 0.0635\n",
      "Epoch [16/40], Step [75/164], Loss: 0.0778\n",
      "Epoch [16/40], Step [100/164], Loss: 0.1340\n",
      "Epoch [16/40], Step [125/164], Loss: 0.1881\n",
      "Epoch [16/40], Step [150/164], Loss: 0.2923\n",
      "Train Average Loss:0.1428\n",
      "Start validation #16\n",
      "Validation #16  Average Loss: 0.2035, mIoU: 0.4571\n",
      "Best performance at epoch: 16\n",
      "Save model in ./saved\n",
      "Epoch [17/40], Step [25/164], Loss: 0.1409\n",
      "Epoch [17/40], Step [50/164], Loss: 0.1058\n",
      "Epoch [17/40], Step [75/164], Loss: 0.1234\n",
      "Epoch [17/40], Step [100/164], Loss: 0.1542\n",
      "Epoch [17/40], Step [125/164], Loss: 0.1672\n",
      "Epoch [17/40], Step [150/164], Loss: 0.1419\n",
      "Train Average Loss:0.1389\n",
      "Start validation #17\n",
      "Validation #17  Average Loss: 0.2144, mIoU: 0.4494\n",
      "Epoch [18/40], Step [25/164], Loss: 0.0946\n",
      "Epoch [18/40], Step [50/164], Loss: 0.2417\n",
      "Epoch [18/40], Step [75/164], Loss: 0.0723\n",
      "Epoch [18/40], Step [100/164], Loss: 0.1024\n",
      "Epoch [18/40], Step [125/164], Loss: 0.1918\n",
      "Epoch [18/40], Step [150/164], Loss: 0.1263\n",
      "Train Average Loss:0.1382\n",
      "Start validation #18\n",
      "Validation #18  Average Loss: 0.2039, mIoU: 0.4530\n",
      "Epoch [19/40], Step [25/164], Loss: 0.1207\n",
      "Epoch [19/40], Step [50/164], Loss: 0.0892\n",
      "Epoch [19/40], Step [75/164], Loss: 0.1263\n",
      "Epoch [19/40], Step [100/164], Loss: 0.1465\n",
      "Epoch [19/40], Step [125/164], Loss: 0.1590\n",
      "Epoch [19/40], Step [150/164], Loss: 0.1520\n",
      "Train Average Loss:0.1323\n",
      "Start validation #19\n",
      "Validation #19  Average Loss: 0.2038, mIoU: 0.4557\n",
      "Epoch [20/40], Step [25/164], Loss: 0.0595\n",
      "Epoch [20/40], Step [50/164], Loss: 0.1989\n",
      "Epoch [20/40], Step [75/164], Loss: 0.0701\n",
      "Epoch [20/40], Step [100/164], Loss: 0.1463\n",
      "Epoch [20/40], Step [125/164], Loss: 0.1365\n",
      "Epoch [20/40], Step [150/164], Loss: 0.1115\n",
      "Train Average Loss:0.1205\n",
      "Start validation #20\n",
      "Validation #20  Average Loss: 0.2030, mIoU: 0.4622\n",
      "Best performance at epoch: 20\n",
      "Save model in ./saved\n",
      "Epoch [21/40], Step [25/164], Loss: 0.0984\n",
      "Epoch [21/40], Step [50/164], Loss: 0.1322\n",
      "Epoch [21/40], Step [75/164], Loss: 0.1212\n",
      "Epoch [21/40], Step [100/164], Loss: 0.0813\n",
      "Epoch [21/40], Step [125/164], Loss: 0.1772\n",
      "Epoch [21/40], Step [150/164], Loss: 0.0634\n",
      "Train Average Loss:0.1123\n",
      "Start validation #21\n",
      "Validation #21  Average Loss: 0.2016, mIoU: 0.4653\n",
      "Best performance at epoch: 21\n",
      "Save model in ./saved\n",
      "Epoch [22/40], Step [25/164], Loss: 0.0798\n",
      "Epoch [22/40], Step [50/164], Loss: 0.1430\n",
      "Epoch [22/40], Step [75/164], Loss: 0.0990\n",
      "Epoch [22/40], Step [100/164], Loss: 0.1276\n",
      "Epoch [22/40], Step [125/164], Loss: 0.0960\n",
      "Epoch [22/40], Step [150/164], Loss: 0.0799\n",
      "Train Average Loss:0.1042\n",
      "Start validation #22\n",
      "Validation #22  Average Loss: 0.2019, mIoU: 0.4664\n",
      "Epoch [23/40], Step [25/164], Loss: 0.0771\n",
      "Epoch [23/40], Step [50/164], Loss: 0.0948\n",
      "Epoch [23/40], Step [75/164], Loss: 0.1331\n",
      "Epoch [23/40], Step [100/164], Loss: 0.1417\n",
      "Epoch [23/40], Step [125/164], Loss: 0.0775\n",
      "Epoch [23/40], Step [150/164], Loss: 0.1222\n",
      "Train Average Loss:0.1077\n",
      "Start validation #23\n",
      "Validation #23  Average Loss: 0.2026, mIoU: 0.4578\n",
      "Epoch [24/40], Step [25/164], Loss: 0.1250\n",
      "Epoch [24/40], Step [50/164], Loss: 0.1156\n",
      "Epoch [24/40], Step [75/164], Loss: 0.0886\n",
      "Epoch [24/40], Step [100/164], Loss: 0.0816\n",
      "Epoch [24/40], Step [125/164], Loss: 0.1770\n",
      "Epoch [24/40], Step [150/164], Loss: 0.0684\n",
      "Train Average Loss:0.1094\n",
      "Start validation #24\n",
      "Validation #24  Average Loss: 0.2002, mIoU: 0.4634\n",
      "Best performance at epoch: 24\n",
      "Save model in ./saved\n",
      "Epoch [25/40], Step [25/164], Loss: 0.0882\n",
      "Epoch [25/40], Step [50/164], Loss: 0.0807\n",
      "Epoch [25/40], Step [75/164], Loss: 0.1065\n",
      "Epoch [25/40], Step [100/164], Loss: 0.0948\n",
      "Epoch [25/40], Step [125/164], Loss: 0.0876\n",
      "Epoch [25/40], Step [150/164], Loss: 0.2379\n",
      "Train Average Loss:0.1160\n",
      "Start validation #25\n",
      "Validation #25  Average Loss: 0.2060, mIoU: 0.4614\n",
      "Epoch [26/40], Step [25/164], Loss: 0.1102\n",
      "Epoch [26/40], Step [50/164], Loss: 0.1191\n",
      "Epoch [26/40], Step [75/164], Loss: 0.0723\n",
      "Epoch [26/40], Step [100/164], Loss: 0.0832\n",
      "Epoch [26/40], Step [125/164], Loss: 0.1345\n",
      "Epoch [26/40], Step [150/164], Loss: 0.0843\n",
      "Train Average Loss:0.1006\n",
      "Start validation #26\n",
      "Validation #26  Average Loss: 0.2014, mIoU: 0.4597\n",
      "Epoch [27/40], Step [25/164], Loss: 0.0645\n",
      "Epoch [27/40], Step [50/164], Loss: 0.0887\n",
      "Epoch [27/40], Step [75/164], Loss: 0.1012\n",
      "Epoch [27/40], Step [100/164], Loss: 0.1125\n",
      "Epoch [27/40], Step [125/164], Loss: 0.0962\n",
      "Epoch [27/40], Step [150/164], Loss: 0.1675\n",
      "Train Average Loss:0.1051\n",
      "Start validation #27\n",
      "Validation #27  Average Loss: 0.1964, mIoU: 0.4731\n",
      "Best performance at epoch: 27\n",
      "Save model in ./saved\n",
      "Epoch [28/40], Step [25/164], Loss: 0.0737\n",
      "Epoch [28/40], Step [50/164], Loss: 0.0827\n",
      "Epoch [28/40], Step [75/164], Loss: 0.0909\n",
      "Epoch [28/40], Step [100/164], Loss: 0.0772\n",
      "Epoch [28/40], Step [125/164], Loss: 0.1125\n",
      "Epoch [28/40], Step [150/164], Loss: 0.0890\n",
      "Train Average Loss:0.0877\n",
      "Start validation #28\n",
      "Validation #28  Average Loss: 0.1958, mIoU: 0.4795\n",
      "Best performance at epoch: 28\n",
      "Save model in ./saved\n",
      "Epoch [29/40], Step [25/164], Loss: 0.1278\n",
      "Epoch [29/40], Step [50/164], Loss: 0.0804\n",
      "Epoch [29/40], Step [75/164], Loss: 0.1094\n",
      "Epoch [29/40], Step [100/164], Loss: 0.1396\n",
      "Epoch [29/40], Step [125/164], Loss: 0.1770\n",
      "Epoch [29/40], Step [150/164], Loss: 0.0598\n",
      "Train Average Loss:0.1157\n",
      "Start validation #29\n",
      "Validation #29  Average Loss: 0.1977, mIoU: 0.4727\n",
      "Epoch [30/40], Step [25/164], Loss: 0.0714\n",
      "Epoch [30/40], Step [50/164], Loss: 0.0761\n",
      "Epoch [30/40], Step [75/164], Loss: 0.0931\n",
      "Epoch [30/40], Step [100/164], Loss: 0.1078\n",
      "Epoch [30/40], Step [125/164], Loss: 0.0943\n",
      "Epoch [30/40], Step [150/164], Loss: 0.0692\n",
      "Train Average Loss:0.0853\n",
      "Start validation #30\n",
      "Validation #30  Average Loss: 0.1971, mIoU: 0.4664\n",
      "Epoch [31/40], Step [25/164], Loss: 0.1077\n",
      "Epoch [31/40], Step [50/164], Loss: 0.0678\n",
      "Epoch [31/40], Step [75/164], Loss: 0.0821\n",
      "Epoch [31/40], Step [100/164], Loss: 0.0857\n",
      "Epoch [31/40], Step [125/164], Loss: 0.0664\n",
      "Epoch [31/40], Step [150/164], Loss: 0.0988\n",
      "Train Average Loss:0.0848\n",
      "Start validation #31\n",
      "Validation #31  Average Loss: 0.2122, mIoU: 0.4707\n",
      "Epoch [32/40], Step [25/164], Loss: 0.0800\n",
      "Epoch [32/40], Step [50/164], Loss: 0.1185\n",
      "Epoch [32/40], Step [75/164], Loss: 0.1194\n",
      "Epoch [32/40], Step [100/164], Loss: 0.0866\n",
      "Epoch [32/40], Step [125/164], Loss: 0.1096\n",
      "Epoch [32/40], Step [150/164], Loss: 0.0581\n",
      "Train Average Loss:0.0954\n",
      "Start validation #32\n",
      "Validation #32  Average Loss: 0.1990, mIoU: 0.4728\n",
      "Epoch [33/40], Step [25/164], Loss: 0.0647\n",
      "Epoch [33/40], Step [50/164], Loss: 0.0578\n",
      "Epoch [33/40], Step [75/164], Loss: 0.0716\n",
      "Epoch [33/40], Step [100/164], Loss: 0.0591\n",
      "Epoch [33/40], Step [125/164], Loss: 0.0755\n",
      "Epoch [33/40], Step [150/164], Loss: 0.0768\n",
      "Train Average Loss:0.0676\n",
      "Start validation #33\n",
      "Validation #33  Average Loss: 0.2116, mIoU: 0.4714\n",
      "Epoch [34/40], Step [25/164], Loss: 0.1148\n",
      "Epoch [34/40], Step [50/164], Loss: 0.1012\n",
      "Epoch [34/40], Step [75/164], Loss: 0.0782\n",
      "Epoch [34/40], Step [100/164], Loss: 0.0667\n",
      "Epoch [34/40], Step [125/164], Loss: 0.0560\n",
      "Epoch [34/40], Step [150/164], Loss: 0.0813\n",
      "Train Average Loss:0.0830\n",
      "Start validation #34\n",
      "Validation #34  Average Loss: 0.2001, mIoU: 0.4760\n",
      "Epoch [35/40], Step [25/164], Loss: 0.1071\n",
      "Epoch [35/40], Step [50/164], Loss: 0.0676\n",
      "Epoch [35/40], Step [75/164], Loss: 0.0996\n",
      "Epoch [35/40], Step [100/164], Loss: 0.1017\n",
      "Epoch [35/40], Step [125/164], Loss: 0.0518\n",
      "Epoch [35/40], Step [150/164], Loss: 0.0728\n",
      "Train Average Loss:0.0834\n",
      "Start validation #35\n",
      "Validation #35  Average Loss: 0.1969, mIoU: 0.4713\n",
      "Epoch [36/40], Step [25/164], Loss: 0.0546\n",
      "Epoch [36/40], Step [50/164], Loss: 0.0462\n",
      "Epoch [36/40], Step [75/164], Loss: 0.0658\n",
      "Epoch [36/40], Step [100/164], Loss: 0.0472\n",
      "Epoch [36/40], Step [125/164], Loss: 0.0738\n",
      "Epoch [36/40], Step [150/164], Loss: 0.0852\n",
      "Train Average Loss:0.0621\n",
      "Start validation #36\n",
      "Validation #36  Average Loss: 0.2046, mIoU: 0.4793\n",
      "Epoch [37/40], Step [25/164], Loss: 0.0712\n",
      "Epoch [37/40], Step [50/164], Loss: 0.0677\n",
      "Epoch [37/40], Step [75/164], Loss: 0.0838\n",
      "Epoch [37/40], Step [100/164], Loss: 0.0908\n",
      "Epoch [37/40], Step [125/164], Loss: 0.0641\n",
      "Epoch [37/40], Step [150/164], Loss: 0.0826\n",
      "Train Average Loss:0.0767\n",
      "Start validation #37\n",
      "Validation #37  Average Loss: 0.2049, mIoU: 0.4776\n",
      "Epoch [38/40], Step [25/164], Loss: 0.0685\n",
      "Epoch [38/40], Step [50/164], Loss: 0.0699\n",
      "Epoch [38/40], Step [75/164], Loss: 0.1095\n",
      "Epoch [38/40], Step [100/164], Loss: 0.0737\n",
      "Epoch [38/40], Step [125/164], Loss: 0.0764\n",
      "Epoch [38/40], Step [150/164], Loss: 0.0881\n",
      "Train Average Loss:0.0810\n",
      "Start validation #38\n",
      "Validation #38  Average Loss: 0.2080, mIoU: 0.4759\n",
      "Epoch [39/40], Step [25/164], Loss: 0.0633\n",
      "Epoch [39/40], Step [50/164], Loss: 0.0494\n",
      "Epoch [39/40], Step [75/164], Loss: 0.1099\n",
      "Epoch [39/40], Step [100/164], Loss: 0.0768\n",
      "Epoch [39/40], Step [125/164], Loss: 0.0536\n",
      "Epoch [39/40], Step [150/164], Loss: 0.0916\n",
      "Train Average Loss:0.0741\n",
      "Start validation #39\n",
      "Validation #39  Average Loss: 0.2044, mIoU: 0.4721\n",
      "Epoch [40/40], Step [25/164], Loss: 0.0636\n",
      "Epoch [40/40], Step [50/164], Loss: 0.1293\n",
      "Epoch [40/40], Step [75/164], Loss: 0.0522\n",
      "Epoch [40/40], Step [100/164], Loss: 0.0647\n",
      "Epoch [40/40], Step [125/164], Loss: 0.0597\n",
      "Epoch [40/40], Step [150/164], Loss: 0.0691\n",
      "Train Average Loss:0.0731\n",
      "Start validation #40\n",
      "Validation #40  Average Loss: 0.2142, mIoU: 0.4707\n",
      "\n",
      "\n",
      "time:11724.66204047203\n"
     ]
    }
   ],
   "source": [
    "#### import time\n",
    "start = time.time()\n",
    "train(cfg.num_epochs, model, train_loader, val_loader, custom, optimizer, saved_dir, val_every, device)\n",
    "print(f'\\n\\ntime:{time.time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d114be-9aa8-468e-a978-fabf1e679acd",
   "metadata": {},
   "source": [
    "# load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e874c493-716f-423d-8593-625e06ff993f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model 저장된 경로\n",
    "model_path = f'./saved/{cfg.model_name}_{cfg.encoders}_{cfg.encode_weights}_best_model.pt'\n",
    "\n",
    "# best model 불러오기\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# 추론을 실행하기 전에는 반드시 설정 (batch normalization, dropout 를 평가 모드로 설정)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b4002e-65ce-42cb-bc09-1f79403d4fcd",
   "metadata": {},
   "source": [
    "# test for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "020b1ea8-e7bf-480a-a9c5-d520ac1390dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, device):\n",
    "    size = 256\n",
    "    transform = A.Compose([A.Resize(256, 256)])\n",
    "    print('Start prediction.')\n",
    "    model.eval()\n",
    "    \n",
    "    file_name_list = []\n",
    "    preds_array = np.empty((0, size*size), dtype=np.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (imgs, image_infos) in enumerate(test_loader):\n",
    "\n",
    "            # inference (512 x 512)\n",
    "            outs = model(torch.stack(imgs).to(device))\n",
    "            oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "            \n",
    "            # resize (256 x 256)\n",
    "            temp_mask = []\n",
    "            for img, mask in zip(np.stack(imgs), oms):\n",
    "                transformed = transform(image=img, mask=mask)\n",
    "                mask = transformed['mask']\n",
    "                temp_mask.append(mask)\n",
    "\n",
    "            oms = np.array(temp_mask)\n",
    "            \n",
    "            oms = oms.reshape([oms.shape[0], size*size]).astype(int)\n",
    "            preds_array = np.vstack((preds_array, oms))\n",
    "            \n",
    "            file_name_list.append([i['file_name'] for i in image_infos])\n",
    "    print(\"End prediction.\")\n",
    "    file_names = [y for x in file_name_list for y in x]\n",
    "    \n",
    "    return file_names, preds_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6cd5da-d6a9-4d0c-b5fd-b1a802283637",
   "metadata": {},
   "source": [
    "# submission.csv 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcab6099-70e3-4933-a858-e604d8e407c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prediction.\n",
      "End prediction.\n"
     ]
    }
   ],
   "source": [
    "# sample_submisson.csv 열기\n",
    "submission = pd.read_csv('./submission/sample_submission.csv', index_col=None)\n",
    "\n",
    "# test set에 대한 prediction\n",
    "file_names, preds = test(model, test_loader, device)\n",
    "\n",
    "# PredictionString 대입\n",
    "for file_name, string in zip(file_names, preds):\n",
    "    submission = submission.append({\"image_id\" : file_name, \"PredictionString\" : ' '.join(str(e) for e in string.tolist())}, \n",
    "                                   ignore_index=True)\n",
    "\n",
    "# submission.csv로 저장\n",
    "submission.to_csv(f\"./submission/{cfg.model_name}_{cfg.encoders}_{cfg.encode_weights}_(pretrained).csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6331e0d-8103-4694-a258-5470e8cea5ea",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63c7b820-3fa7-40f2-8a0d-7e5181ede772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classname(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "class CustomDataLoader2(Dataset):\n",
    "    \"\"\"COCO format\"\"\"\n",
    "    def __init__(self, data_dir, mode = 'train', transform = None):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(data_dir)\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        # dataset이 index되어 list처럼 동작\n",
    "        image_id = self.coco.getImgIds(imgIds=index)\n",
    "        image_infos = self.coco.loadImgs(image_id)[0]\n",
    "        \n",
    "        # cv2 를 활용하여 image 불러오기\n",
    "        images = cv2.imread(os.path.join(dataset_path, image_infos['file_name']))\n",
    "        images = cv2.cvtColor(images, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        images /= 255.0\n",
    "        images = images.astype(np.uint8)\n",
    "        \n",
    "        if (self.mode in ('train', 'val')):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=image_infos['id'])\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "            # Load the categories in a variable\n",
    "            cat_ids = self.coco.getCatIds()\n",
    "            cats = self.coco.loadCats(cat_ids)\n",
    "\n",
    "            # masks : size가 (height x width)인 2D\n",
    "            # 각각의 pixel 값에는 \"category id + 1\" 할당\n",
    "            # Background = 0\n",
    "            masks = np.zeros((image_infos[\"height\"], image_infos[\"width\"]))\n",
    "            # Unknown = 1, General trash = 2, ... , Cigarette = 11\n",
    "            for i in range(len(anns)):\n",
    "                className = get_classname(anns[i]['category_id'], cats)\n",
    "                pixel_value = category_names.index(className)\n",
    "                masks = np.maximum(self.coco.annToMask(anns[i])*pixel_value, masks)\n",
    "            masks = masks.astype(np.uint8)\n",
    "            \n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images, mask=masks)\n",
    "                images = transformed[\"image\"]\n",
    "                masks = transformed[\"mask\"]\n",
    "            \n",
    "            return images, masks, image_infos\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            # transform -> albumentations 라이브러리 활용\n",
    "            if self.transform is not None:\n",
    "                transformed = self.transform(image=images)\n",
    "                images = transformed[\"image\"]\n",
    "            \n",
    "            return images, image_infos\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        # 전체 dataset의 size를 return\n",
    "        return len(self.coco.getImgIds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63b66077-a63c-4846-9205-3129d5206dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_test_loader(test_loader, model):\n",
    "    for imgs, masks in test_loader:\n",
    "        temp_images = imgs\n",
    "        \n",
    "        model.eval()\n",
    "        outs = model(torch.stack(temp_images).to(device))\n",
    "        oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "        break\n",
    "\n",
    "    for idx, img in enumerate(temp_images):\n",
    "        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 12))\n",
    "\n",
    "        print('image shape:', list(temp_images[idx].shape))\n",
    "        print('mask shape: ', list(oms[idx].shape))\n",
    "        print('Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[idx]))])\n",
    "\n",
    "        ax1.imshow(temp_images[idx].permute([1,2,0]))\n",
    "        ax1.grid(False)\n",
    "\n",
    "        ax2.imshow(oms[idx])\n",
    "        ax2.grid(False)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "def imshow_train_loader(train_loader, model,w_cnt):\n",
    "    cnt = 0  \n",
    "    for imgs, masks, image_infos in train_loader:\n",
    "        temp_images = imgs\n",
    "        image_infos = image_infos[0]\n",
    "        temp_masks = masks\n",
    "    \n",
    "        model.eval()\n",
    "        outs = model(torch.stack(temp_images).to(device))\n",
    "        oms = torch.argmax(outs.squeeze(), dim=1).detach().cpu().numpy()\n",
    "        cnt+=1\n",
    "        if w_cnt == cnt:\n",
    "            break\n",
    "\n",
    "    for idx, img in enumerate(temp_images):\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(24, 24))\n",
    "    \n",
    "        print('답안,Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(temp_masks[idx]))])\n",
    "        print('학습한거,Unique values, category of transformed mask : \\n', [{int(i),category_names[int(i)]} for i in list(np.unique(oms[idx]))])\n",
    "\n",
    "        ax1.imshow(temp_images[idx].permute([1,2,0]))\n",
    "        ax1.grid(False)\n",
    "\n",
    "        ax2.imshow(oms[idx])\n",
    "        ax2.grid(False)\n",
    "\n",
    "        ax3.imshow(temp_masks[idx])\n",
    "        ax3.grid(False)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c19351b-60fa-4c04-8499-19880f3990be",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff315802-d612-45f9-911b-918bdd566ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.37s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.VerticalFlip(p=0.5),\n",
    "                            A.RandomRotate90(),\n",
    "                            A.transforms.ColorJitter(),\n",
    "                            A.OneOf([\n",
    "                                A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                                A.GridDistortion(p=0.5),\n",
    "                                A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "                            ], p=0.5),\n",
    "                            A.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225]),\n",
    "    '''\n",
    "\n",
    "EDA_transform = A.Compose([\n",
    "                            A.transforms.CLAHE(),\n",
    "                            ])\n",
    "\n",
    "EDA_dataset = CustomDataLoader2(data_dir=train_path, mode='train', transform=EDA_transform)\n",
    "\n",
    "test_EDA_dataset = CustomDataLoader2(data_dir=test_path, mode='test', transform=EDA_transform)\n",
    "\n",
    "EDA_loader = torch.utils.data.DataLoader(dataset=EDA_dataset, \n",
    "                                           batch_size=8,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n",
    "\n",
    "test_EDA_loader = torch.utils.data.DataLoader(dataset=test_EDA_dataset, \n",
    "                                           batch_size=8,\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=4,\n",
    "                                           collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0fd9709d-2452-4a0c-8ac3-4740ce2b753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow_train_loader(EDA_loader,model,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a167af-3b74-43e1-9bcd-8700cb689363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
